# -*- coding: utf-8 -*-
"""Hw3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1py7ZGgHCmg37W9snLGWz5OrmwZO46kqc

# Імпорти
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Напишіть функцію гіпотези лінійної регресії у векторному вигляді"""


def hypothesis(X, w):
    """
    Обчислює значення функції гіпотези для заданої вибірки та вектора ваг.
    
    Args:
    X (np.ndarray): Матриця ознак розмірності (m, n+1), де m - кількість прикладів у вибірці,
                    n - кількість ознак.
    w (np.ndarray): Вектор ваг розмірності (n+1, 1).
    
    Returns:
    np.ndarray: Вектор передбачень розмірності (m, 1).
    """
    return X @ w


"""# Створіть функцію для обчислення функції втрат у векторному вигляді"""


def loss_function(X, y, w):
    """
    Обчислює значення функції втрат для заданої вибірки та вектора ваг.
    
    Args:
    X (np.ndarray): Матриця ознак розмірності (m, n+1), де m - кількість прикладів у вибірці,
                    n - кількість ознак.
    y (np.ndarray): Вектор правильних відповідей розмірності (m, ).
    w (np.ndarray): Вектор ваг розмірності (n+1, 1).
    
    Returns:
    float: Значення функції втрат.
    """
    m = X.shape[0]
    return ((hypothesis(X, w) - y) ** 2).sum() / (2 * m)


"""# Реалізуйте один крок градієнтного спуску"""


def gradient_descent_step(w, grad, learning_rate):
    """
    Обчислює новий вектор ваг на основі останнього вектора ваг та градієнта,
    за допомогою кроку градієнтного спуску з заданою швидкістю навчання.
    
    Args:
    w (np.ndarray): Вектор ваг розмірності (n, 1).
    grad (np.ndarray): Вектор градієнту розмірності (n, 1).
    learning_rate (float): Швидкість навчання.
    
    Returns:
    np.ndarray: Новий вектор ваг розмірності (n, 1).
    """
    return w - learning_rate * grad.reshape(w.shape)


"""*Функція граієнту*"""


def gradient(X, y, w):
    """
    Обчислення градієнту функції втрат за лінійної регресії.

    Args:
    X (np.ndarray): Матриця розмірності (m, n+1), де m - кількість прикладів, n - кількість ознак.
    y (np.ndarray): Вектор відповідей розмірності (m, 1).
    w (np.ndarray): Вектор вагів розмірності (n+1, 1).

    Returns:
    np.ndarray: Градієнт функції втрат за вагами розмірності (n+1, 1).
    """
    m = X.shape[0]
    h = hypothesis(X, w)
    grad = X.T @ (h - y)
    return grad / m


"""# Знайдіть найкращі параметри w для датасету прогнозуючу ціну на будинок залежно від площі, кількості ванних кімнат та кількості спалень

*Завантаження даних*
"""

data = pd.read_csv("/content/sample_data/Housing.csv")

"""*Вибір колонок для використання*"""

X = data[["area", "bedrooms", "bathrooms"]].values
y = data["price"].values

"""*Нормалізація даних*"""

X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_norm = (X - X_mean) / X_std

y_mean = y.mean()
y_std = y.std()
y_norm = (y - y_mean) / y_std

"""*Початкові значення вагів*"""

m = X_norm.shape[0]

# Додати стовпець з одиницями до матриці X_norm
X_norm = np.hstack((np.ones((m, 1)), X_norm))

n = X_norm.shape[1] - 1

w = np.zeros((n + 1, 1))

"""*Гіперпараметри*"""

learning_rate = 0.1
num_iterations = 1000

"""*Градієнтний спуск*"""

best_loss = float('inf')
best_weights = None
loss_history = []

for i in range(num_iterations):
    # Обчислення градієнту
    grad = gradient(X_norm, y_norm, w)

    # Оновлення вагів
    w = w - learning_rate * grad

    # Обчислення функції втрат
    loss = loss_function(X_norm, y_norm, w)
    loss_history.append(loss)

    # Зберігання кращих параметрів
    if loss < best_loss:
        best_loss = loss
        best_weights = w

    # Вивід функції втрат на кожній ітерації
    if i % 100 == 0:
        print(f"iteration {i + 1}: loss = {loss}")
print(f"Best weights: {best_weights}")
print(f"Best loss: {best_loss}")

"""# Візуалізація результатів"""

loss_history

plt.plot(loss_history)
plt.title('Функція втрат')
plt.xlabel('Ітерація')
plt.ylabel('Значення')
plt.show()

"""# Знайдіть ці ж параметри за допомогою аналітичного рішення"""

# Нормалізація вектора відповідей
y_norm = (y - y.mean()) / y.std()

# Обчислення аналітичного рішення
w_analytical = np.linalg.inv(X_norm.T @ X_norm) @ X_norm.T @ y_norm

# Вивід кращих параметрів
print(f"Кращі параметри: {w_analytical.flatten()}")

"""# Порівняйте отримані результати"""

# Обчислюємо передбачення лінійної регресії, навченої за допомогою градієнтного спуску
y_pred = hypothesis(X_norm, w)

# Обчислюємо передбачення лінійної регресії, знайденої за допомогою аналітичного рішення
y_pred_analytical = hypothesis(X_norm, w_analytical)

# Обчислюємо середньоквадратичну помилку для лінійної регресії, навченої за допомогою градієнтного спуску
mse = np.mean((y_norm - y_pred) ** 2)

# Обчислюємо середньоквадратичну помилку для лінійної регресії, знайденої за допомогою аналітичного рішення
mse_analytical = np.mean((y_norm - y_pred_analytical) ** 2)

print("Середньоквадратична помилка для лінійної регресії, навченої за допомогою градієнтного спуску: ", mse)
print("Середньоквадратична помилка для лінійної регресії, знайденої за допомогою аналітичного рішення: ",
      mse_analytical)
